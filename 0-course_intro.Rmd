---
title: "Wittenberg University - Master of Science in Analytics"
subtitle: "ANLT 510 - Advanced Statistics and Modeling"
author: "Course Overview"
date: "`r format(Sys.Date(), format = '%d %b %Y')`"
output: 
  slidy_presentation:
    smart: no
    fig_caption: yes
graphics: yes
params:
  day1_date: "Oct 15"
  day2_date: "Oct 16"
  day3_date: "Nov 12"
---

# Course Overview

```{r child="resources/R/setup.Rmd"}
```


## Basics Course Info

- Course title: ANLT 510: Advanced Statistics and Modeling
- Meeting times: Fri 15 Oct, Sat 16 Oct  & Fri 12 Nov | 8:30 am – 5:00 pm
- Instructor: Jason Freels, PhD <smaller><span class="explain">`r fontawesome::fa("external-link-alt")`</span><span class="tooltip">Contact Info<br/>Email: jasonkfreels@gmail.com<br/>Phone: 937.430.6619 (cell)</span></smaller>
- Office hours: Mon/Thurs 4:00p – 5:00p <smaller><span class="explain">`r fontawesome::fa("external-link-alt")`</span><span class="tooltip">Note: These are designated office hours in which I guarantee that I can be available.  Feel free contact me at any time outside these hours - and I will get back to you as soon as I can.</span></smaller>

## Course Overview

- In the interdisciplinary field of analytics, a good understanding of statistical theory is important for drawing insights from data  

- Many modern analytics problems involve the application of advanced statistical methods and cannot be performed without the use of software

- Therefore, this course presents the theory behind various statistical methods in combination with the programming skills required to implement them on data

- Topics to be discussed include:

    + Methods of collecting and accessing data
    + Techniques for manipulating, summarizing, and visualizing data
    + Selecting appropriate methods to analyze data – to detect trends, to check for effects, to make predictions and decisions
    + Constructing statistical models to estimate and predict real-world phenomena
    + Communicating results in a reproducible manner

## Course Pre-requisites & Co-requisites

- There are no formal pre-requisites, but a basic working knowledge of the fundamental concepts and methods of statistics is presumed

- Because each student comes into the course with a background in statistics experience, I’ll review several important introductory stat concepts and methods that are relevant to statistical modeling and data-based analytics

- There are no formal course co-requisites, but the content presented in this course will overlap in spirit and content with several other courses in the program: Data Mining; Descriptive, Predictive and Prescriptive Analytics; Data Visualization; and Data Management.

## Course learning objectives

- Reinforce fundamental introductory statistical concepts and methods:

    + Connection between data collection and scope of conclusions
    + Common visual and numerical summaries, and what to look for in such summaries
    + Sampling distributions
    + Confidence intervals and significance tests
    + Limitations and abuses of formal inference
    + Understand the statistical modeling process:
    + Choosing appropriate models
    + Fitting models with technology
    + Assessing model fit
    + Checking conditions for reliable use of models
    + Using and applying models in context


# Course Overview - Format

## Resources & Software

- The text for this course is Introduction to Statistical Learning (with applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani

    + E-textbook and hard-copies of the text can be purchased from Amazon, however the authors have made a PDF version of the text available.  I've provided a copy of this PDF version here for you to download
    + In addition, the authors have provided slides, videos and an R package to accompany the text

- You'll note that to complete the course, I've tasked you with specific reading chapters from the text each week.  If you would like to use the slides and videos, that's fine but they should be used as an additional resource to complement reading the text - not a substitute for it.  

- Lastly, the following resources may be helpful:

    + AFIT Data Science Lab R Programming Guide, https://afit-r.github.io/
    + Hadley Wickham, Advanced R, 2014, http://adv-r.had.co.nz/
    + R Markdown: The Definitive Guide https://bookdown.org/yihui/rmarkdown/
    + During the in-class sessions (10/15 & 10/16) I'll use the slides to cover chapters 1-4 of the ISLR textbook to get you set to complete Assignment #1.  These sessions will be comprised primarily of lectures, mixed with R coding demonstrations.

- Throughout the course we will use the R Project for Statistical Computing and the RStudio Integrated Development Environment.  A number of R Packages will be used throughout the course, students will be able to download and install them on the fly. 

## How to follow along with the course

- During these first two days of in-person learning we'll cover the first 4 chapters in the ISLR text 

- Along with the slides I'll provide code chunks so that you can run the code on your machine

- We'll also walk through the first homework assignment

- The goal will be to ensure that you're ready to complete the remaining three homework assignments 

# Why R?

## A language for data analysis and graphics

- This definition of R was used by Ross Ihaka and Robert Gentleman in the title of their 1996 [paper](https://www.stat.auckland.ac.nz/~ihaka/downloads/R-paper.pdf) outlining their experience of designing and implementing the R software. 

- It’s safe to say this remains the essence of what R is; however, it’s tough to encapsulate such a diverse programming language into a single phrase.

- The R programming language has become one of the most widely used tools for statistics and data science

- Its application runs the gamut from data pre-processing, cleaning, web scraping and visualization to a wide range of analytic tasks such as computational statistics, econometrics, optimization, and natural language processing

- Is being used by many organizations such as Google, Facebook, New York Times, Twitter, Etsy, Department of Defense, and even in presidential political campaigns

## So what makes R such a popular tool?

- R is an *open source* software created over 20 years ago by Ihaka and Gentleman at the University of Auckland, New Zealand. 

    + Its lineage goes back to the S programming language created by John Chambers out of Bell Labs back in the 1970s
    + R is actually a combination of S with lexical scoping semantics inspired by Scheme
    + The S language has been a popular vehicle for research in statistical methodology, and R provides an *open source* route to participate in that activity

Although the history of S and R is interesting[^peng], the principal artifact to observe is that R is an *open source* software. Although some contest that open-source software is merely a "craze" [^ORMS], most evidence suggests that open-source is here to stay and represents a *new*[^newness] norm for programming languages. Open-source software such as R blurs the distinction between developer and user which provides the ability to extend and modify the analytic functionality to your, or your organization's needs. The data analysis process is rarely restricted to just a handful of tasks with predictable input and outputs that can be pre-defined by a fixed user interface as is common in proprietary software. Rather, as previously mentioned in the introduction, data analysis includes unique, different, and often multiple requirements regarding the specific tasks involved.  Open source software allows more flexibility for you, the data analyst, to manage how data are being transformed, manipulated, and modeled "under the hood" of software rather than relying on "stiff" point and click software interfaces. Open source also allows you to operate on every major platform rather than be restricted to what your personal budget allows or the idiosyncratic purchases of organizations.

This invariably leads to new expectations for data analysts; however, organizations are proving to greatly value the increased technical abilities of open source data analysts as evidenced by a recent O'Reilly survey revealing that data analysts focusing on open source technologies make more money than those still dealing in proprietary technologies.

## R is extremely flexible

- Another benefit of open source is that anybody can access the source code, modify and improve it

    + Many excellent programmers contribute to improving existing R code and developing new capabilities
    + Researchers from all walks of life (academic institutions, industry, and focus groups such as [RStudio](https://www.rstudio.com) and [rOpenSci](https://ropensci.org/packages/)) are contributing to advancements of R's capabilities and best practices
    + This has resulted in powerful tools that advance both statistical and non-statistical modeling capabilities that are taking data analysis to new levels

Many researchers in academic institutions are using and developing R code to develop the latest techniques in statistics and machine learning. As part of their research, they often publish an R package to accompany their research articles[^r_journals]. This provides immediate access to the latest analytic techniques and implementations. And this research is not soley focused on generalized algorithms as many new capabilities are in the form of advancing analytic algorithms for tasks in specific domains. A quick assessment of the different [task domains](https://cran.r-project.org/web/views/) for which code is being developed illustrates the wide spectrum - econometrics, finance, chemometrics & computational physics, pharmacokinetics, social sciences, etc.

Powerful tools are also being developed to perform many tasks that greatly aid the data analysis process. This is not limited to just new ways to wrangle your data but also new ways to visualize and communicate data. R packages are now making it easier than ever to create interactive graphics and websites and produce sophisticated html and pdf reports. R packages are also integrating communication with high-performance programming languages such as C, Fortran, and C++ making data analysis more powerful, efficient, and posthaste than ever. 

So although the analytic montra *"use the right tool for the problem"* should always be in our prefrontal cortex, the advancements and flexibility of R is making it the right tool for many problems.

## Community

- The R community is fantastically diverse and engaged

- On a daily basis, the R community generates opportunities and resources for learning about R.  These cover the full spectrum of training - [books](http://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=r+programming), [online courses](https://www.coursera.org/specializations/jhu-data-science), [R user groups](http://blog.revolutionanalytics.com/local-r-groups.html), [workshops](https://www.rstudio.com/resources/training/workshops), [conferences](https://www.r-project.org/conferences.html), etc. And with over 2 million users and developers, finding help and technical expertise is only a simple click away. Support is available through [R mailing lists](https://www.r-project.org/mail.html), Q&A websites[^r_QA], [social media networks](www.twitter.com/search/rstats), and [numerous blogs](http://www.r-bloggers.com/). 

- So now that you know how awesome R is, it's time to learn how to use it.

# Course Overview - Homework

## Assessment

- Students will be assessed based on their completion of assigned homework problems included at the end of each chapter in the text.  Each week students are assigned to read certain chapters from the ISLR textbook and then complete specific exercises from the back of each chapter.  Homework assignments are posted to course Moodle page along with due dates and times (assignments are due each Sunday by 11:59pm). 
- I encourage students to work together to complete the assignments, however each student must submit their own solution.  Moreover, students may use external resources to complete the assignments, but they must be cited. 

## Late Assignments

- Assignments turned in after the assigned date/time but will be considered late. Five (5) points will be deducted from the final assignment grade for each day the assignment is turned in after the assigned due date. I realize that outside events could make it exceedingly difficult for students to turn an assignment in on time.  

- In these circumstances, students should coordinate with the instructor PRIOR TO THE ASSIGNMENT DUE DATE to set up an alternate due date.  If an unforeseen outside event occurs, such that a student could not prior arrangements - it is the instructor's discretion as to whether or not the assignment will be considered late. 

## Reproducible Solutions

- As future data scientists and analysts you should strive to ensure that your results can be reproduced by others.  

- To emphasize this, I require that all student assignment solutions be reproducible

- In this course students accomplish this by submitting their solutions as Rmarkdown files (.Rmd file type)

- Students must ensure that their file can compile as a HTML document, both on their own computer as well as on the instructor's computer

- If a students assignment solution must be modified to compile on the instructor's computer up to 10 points may be deducted from the final assignment grade

- To help student's get started with the formatting an Rmarkdown document, assignment templates will be provided

# Course Overview - Schedule

## Course schedule (tentative)

- Friday 15 October (a.m.) - ISLR Chapter 1

    + Overview of Machine learning model
    + Intro to optimization methods/loss functions
    + Overview of Introductory Statistics 

- Friday 15 October (p.m.) - ISLR Chapter 2

    + ISLR Chapter 1 (Finish)

- Saturday 16 October (a.m.) - ISLR Chapter 3

    + Linear Regression

- Saturday 16 October (p.m.)- ISLR Chapter 4

    + Logistic Regression

# Your Turn!

## Please introduce yourself

- What is your name?
- Where do you currently work?
- What do you hope to get from this program?
- How would you describe your stats background?
- What is your capstone project?

# The landscape of machine Learning Models

## Supervised Vs Unsupervised Learning Algorithms

- You be familiar or have experience in one or more of the following domain areas 

    + Statistical modeling
    + Predictive analytics
    + Data mining
    + Artificial intelligence

- With few exceptions, when these terms are used to discuss a learning algorithm, what's really meant is the field of <b>machine learning</b><smaller><span class="explain">`r fontawesome::fa("external-link-alt")`</span><span class="tooltip">To be pedantic, machine learning is a sub-field of artificial intelligence (AI) - in practice, however, most references to "AI" really imply machine learning</span></smaller>

- Despite being a broad field, most machine learning algorithms can be subdivided in into three main classes

    + Supervised learning algorithms
    + Unsupervised learning algorithms
    + Reinforcement learning

```{r, echo=FALSE, fig.cap="Classes of Machine learning algorithms with applications (reference: https://medium.com/@sanchittanwar75/introduction-to-machine-learning-and-deep-learning-bd25b792e488)" }
knitr::include_graphics(find_resource("images","ml_subs.png"))
```

- Supervised learning algorithms

    + Pertain to data sets that contain both outputs (responses) **AND** inputs (features/factors/predictors) 
    + Describe the relationship between the input(s) and the output(s) 
    + Can be used predict new output(s) given new inputs(s)
    + ***Basic idea***: supervised algorithms "learn" the functional relationship between a set of inputs and the associated output(s)

- Unsupervised learning algorithms

    + Pertain to data that includes **ONLY** inputs (features) 
    + Partition (cluster) the data in a meaningful way
    + ***Basic idea***: We DO NOT HAVE outputs 
    + Examples of when an unsupervised learning algorithm would be used

        - Cluster (or partition) customers according to their buying habits for the purpose of targeted advertising
        - Detect anomalous transactions or faulty pieces of hardware
        - Use association mining to identify items that frequently occur together (i.e. basket analysis) 

# Supervised learning algorithms can be divided into two main classes

## Regression algorithms: when the output/response variable is numeric and continuous

- Example applications of regression algorithms 

    + Housing or stock prices
    + Weather analysis
    + Time series forecasting

- The following algorithms can be used to model (describe) the relationship between inputs and outputs when the outputs are numeric and continuous

    + Linear regression
    + Polynomial regression
    + Ridge regression
    + Lasso regression
    + ElasticNets
    + Decision trees
    + Random forests
    + Gradient boosting methods
    + Neural networks

## Classification algorithms: when the output/response variable is discrete or categorical

- Classification is focused on predicting/labeling a discrete output (categories)

- There can be more that two (yes/no) categories

- Example applications of classification algorithms

    + Estimating a customer's satisfaction with a product they haven't tried before
    + Predicting next month's sales
    + Predicting the outcome of a sporting event
    + Labeling whether a photo contains a cucumber or a zucchini

- The following algorithms can be used to model (describe) the relationship between inputs and outputs when the outputs are discrete or categorical

    + Logistic Regression
    + Multinomial Regression
    + Ordinal Regression
    + Logit Regression
    + Probit Regression
    + Nearest Neighbors
    + Decision Trees
    + Random Forest
    + Gradient Boosting
    + Neural Networks

# Model Complexity vs. Model Accuracy

## The big picture of supervised learning algorithms

- Before moving forward I want to make sure that you have a solid (yet high level) understanding of how supervised learning algorithms work under the hood

    + For any example problem you can think of there exists a "perfect" function $f_{\text{perfect}}$ that perfectly describes the relationship between the inputs and the outputs
    + If we could use $f_{\text{perfect}}$ all of our predictions would be "perfect" (i.e. there would be no errors)
    + We will <focus>almost never</focus> be able to find $f_{\text{perfect}}$ because in the real world the relationship is too complex `r emo::ji("cry")` (sorry)
    + Statistical learning algorithms use the inputs and outputs to "learn" the best representation of $f_{\text{perfect}}$
    + This learned function will not be perfect, therefore let's call it  $f_{\text{imperfect}}$
    + It should be obvious that if more data is available $f_{\text{imperfect}}$ will do a better job representing $f_{\text{perfect}}$
    + What may not be obvious is that all algorithms are not equal -- some will do better than others regardless of how much data is available

## Common elements of supervised learning algorithms 

- All supervised learning algorithms have the following elements

    + A data set that includes one or more inputs (features) and one or more outputs (responses)
    + An assumed form of the model $f_{\text{imperfect}}$ that will represent $f_{\text{perfect}}$
    + A set of parameters that can be used to tweak the shape of $f_{\text{imperfect}}$ (think of these a knob settings)
    + A loss function that "learns" which parameter values result in the form of $f_{\text{imperfect}}$ that best represents $f_{\text{perfect}}$

# Supervised learning Example

## Elements of supervised learning algorithms: #1 Data

- Suppose you're asked to create a model to describe the relationship between one set of inputs and one set of outputs 

    + We'll call the set of inputs `x` and the set of outputs `y`
    + Plotting the inputs and outputs shows the relationship between `x` and `y` in the figure below
    + Looking at the plot we see that this is an "ideal" data set 

```{r simpledata, fig.cap="Plot of some ideal data", echo=FALSE}
library(ggplot2)
# Create a data.frame object
df <- data.frame(x = seq(0, 6, by = 0.5))

# Add a "column" y to the data.frame df
df$y <- 5 * df$x + 3

# plot() opens a new "base" R graphics device
ggplot(df, aes(x,y)) +
  geom_point(colour = "blue", size = 2.5) +
  theme_bw(base_size = 16)
```

## Elements of supervised learning algorithms: #2 an assumed form of $f_{\text{imperfect}}$

- Observing the figure, there doesn't appear to be any uncertainty in the data as each point falls on a straight line

- An obvious choice for a model to describe this data would be a function of the form $y = mx +b$ - the familiar equation for a line

- Adding a plot of the line $y(x) = mx+b$ shows that a "perfect" model exists for our ideal data

```{r perfect, fig.cap='Fitting the ideal data with a "perfect" model', echo=FALSE}
# ggplot() opens a new ggplot2 graphics device
ggplot(df, aes(x,y)) +
  geom_line(colour = "red", size = 1.25) +
  geom_point(colour = "blue", size = 2.5) +
  theme_bw(base_size = 16)+
  ylab(expression(y(x) == m%*%x + b))
```

## Elements of supervised learning algorithms: #3 parameter values

- Now that we've chosen a functional form for $f_{\text{imperfect}}$ we turn our attention to the parameters of the model

    + Every model comes with parameters -- the values assigned to these parameters affect how well $f_{\text{imperfect}}$ represent the relationship between `x` and `y`

    + For our chosen $f_{\text{imperfect}}$ we see that there are two paramters the slope ($m$) and the intercept ($b$)
    
    + The question, of course is what are the correct values of the slope $m$ and the intercept $b$ that best represent the relationship between `x` and `y`

    + For this ideal data set, we can determine these values using our knowledge of straight lines and our ability to read or we could read the value of the intercept directly from the plot as $b = 3$ 

    + Using this value for $b$ we can choose any of the data points and solve for the slope $m = 5$ 

- I know what you're thinking: What does this have to do with machine learning? 

    + So far, nothing - but let's fix that
    + Let's have the machine determine the parameter values that result in the best representation of the relationship between `x` and `y` constraining this relationship the form of $f_{\text{imperfect}}$ that we chose above
    + Finding the best requires the use of optimization methods
    + But before we can do any optimization we first need to answer the question <font color="red"><b>what exactly are we optimizing?</b></font> - the answer is a <u>loss function (aka cost function)</u>. 

## Elements of supervised learning algorithms: #4 loss functions

- A <focus>loss function</focus> helps the machine differentiate between good parameter values and bad parameter values

- Loss functions are a key component in all supervised learning algorithms

- In many cases, you don't have to choose a loss function you use to find the optimal parameter values 
 
    + Rather, it comes as part of a package deal with the modeling approach you choose (i.e. linear regression, logistic regression, etc.).

    + You can, however, come up with your own loss function - so long as it produces meaningful results

# Supervised Learning Example (Cont.)

## Visualizing loss functions

- For our perfect example data, we can choose among several different loss functions

- However, not all of them will return an accurate solution

- In the sections below I walk through the choice of several different loss functions and plot the results

    + A <u>naive</u> loss function as the difference between the observed output and the output returned by the proposed model
    + A good loss function
    + A better loss function

## A Naive loss function

- In this case we use a <u>naive</u> loss function that represents the difference between the observed output and the output returned by the proposed model

- The loss function is expressed as shown below

$$
Loss_{_{naive}}(\mathbf{y},\mathbf{x},m,b) = \sum_{i=1}^N y_i-m\times x_i-b.
$$

- Using this function, loss would simply be defined as the sum of the vertical distances between each observed output $y_i, i = 1,...,n$ and the output returned by the chosen model

- The parameters $m$ and $b$ for the best-fit line correspond to model that has the minimum loss. 

- For our "ideal" data, the points fall on a straight line and we would expect the loss value in this case to be zero

- Thus far, we've chosen a functional form that we believe is a good representation of the data -- and a corresponding loss function

- In the chunk below we define our naive loss function 

```{r naive_loss}
loss_naive <- function(params,x,y) {

  if(length(params) != 2) stop("Params should be a length 2 vector")
  
  m <- params[1]
  b <- params[2]
  
  return(sum(y - m * x - b))

}
```

- Now, let's use the `stats::optim()` function to find the values of $m$ and $b$ that minimize the loss function and result in a model that best-fits the data  

```{r}
optim(par = c(1,1),
      fn = loss_naive,
      x = df$x,
      y = df$y,
      control = list(fnscale = 1))
```

- Looking at these results, it's clear that something isn't right - why?

    + The problem is we created a loss function that isn't minimized at $0$. 
    + We can visualize this loss function by generating a matrix of values for various combinations of $m$ and $b$ and plotting these values where $x = m$, $y = b$, and $z = \text{Loss}$ this plot is shown below.

```{r, out.width='100%', echo=FALSE}
library(plotly)
slope <- seq(0, 10, 0.1)
intercept <- seq(0, 10, 0.1)
loss_n <- matrix(NA, nrow = length(slope), ncol = length(intercept))

for(i in 1:length(slope)) {
  
    for(j in 1:length(intercept)) {
    
        loss_n[i,j] <- sum((df$y - slope[i] * df$x - intercept[j]))
    
    }
  
}

# Generate a 3D surface plot using plotly
p = plot_ly(z = loss_n,
            x = slope, 
            y = intercept, 
            width = 1000, 
            height = 1000) %>% 
    add_surface(contours = list(z = list(show = TRUE,
                                         usecolormap = TRUE,
                                         highlightcolor = "#ff0000",
                                         project = list(z = TRUE))))
```

<center><br>`r p`<br></center>

## A good loss function

- Another possible loss function would involve taking the absolute value of the errors -- we know this function is minimized at zero

$$
Loss_{_{absolute}}(\mathbf{y},\mathbf{x},m,b) = \sum_{i=1}^N \Big\vert y_i-m\times x_i-b\Big\vert.
$$

- Once again let's define our <u>OK</u> loss function 

```{r absolute_loss}
# First define a function to optimize
loss_absolute <- function(params,x,y) {

   if(length(params) != 2) stop("Params should be a length 2 vector")
  
   m <- params[1]
   b <- params[2]
   
   return(sum(abs(y - m * x - b)))

}
```

- Now, let's use the `stats::optim()` function to find the values of $m$ and $b$ that minimize the loss function and result in a model that best-fits the data  

```{r}
optim(par = c(1,1),           # provide starting values for m and b
      fn = loss_absolute,     # define function to optimize
      x = df$x,               # provide values for known parameters
      y = df$y,               # provide values for known parameters
      control = list(fnscale = 1))
```

- Let's visualize this loss function -- like we did before

```{r, out.width='100%', echo=FALSE}
slope <- seq(0, 10, 0.1)
intercept <- seq(0, 10, 0.1)

loss_a <- matrix(NA, nrow = length(slope), ncol = length(intercept))

for(i in 1:length(slope)) {
  
    for(j in 1:length(intercept)) {
    
        loss_a[i,j] <- sum(abs(df$y - slope[i] * df$x - intercept[j]))
    
    }
  
}

# Generate a 3D surface plot using plotly
p = plot_ly(z = loss_a,
            x = slope, 
            y = intercept, 
            width = 1000, 
            height = 1000) %>% 
    add_surface(contours = list(z = list(show = TRUE,
                                         usecolormap = TRUE,
                                         highlightcolor = "#ff0000",
                                         project = list(z = TRUE))))
```

<center><br>`r p`<br></center>

## A better loss function

- The problem is that linear functions are unconstrained

- A better option would be to propose a loss functions that is convex, such as 

$$
Loss_{_{convex}}(\mathbf{y},\mathbf{x},m,b) = \sum_{i=1}^N \Big( y_i-m\times x_i-b\Big)^2.
$$

- Note that we are minimizing the squared distances between the observed values and the proposed model - hence this is called <focus>least squares optimization</focus>

- For the last time let's define our <u>convex</u> loss function 

```{r convex_loss}
loss_convex <- function(params,x,y) {

  if(length(params) != 2) stop("Params should be a length 2 vector")
  
  m <- params[1]
  b <- params[2]
  
  return(sum((y - m * x - b) ^ 2))    

}
```

- Now, let's use the `stats::optim()` function to find the values of $m$ and $b$ that minimize the loss function and result in a model that best-fits the data  

```{r}
optim(par = c(1,1),        # provide starting values for m and b
      fn = loss_convex,    # define function to optimize
      x = df$x,            # provide values for known parameters
      y = df$y,            # provide values for known parameters
      control = list(fnscale = 1))
```

- Let's visualize this loss function -- like we did for the last two 

```{r, out.width='100%', fig.height=9, echo=FALSE, fig.align='center'}
slope <- seq(0, 10, 0.1)
intercept <- seq(0, 10, 0.1)

loss2 <- matrix(NA, nrow = length(slope), ncol = length(intercept))

for(i in 1:length(slope)) {
  
    for(j in 1:length(intercept)) {
    
        loss2[i,j] <- sum((df$y - slope[i] * df$x - intercept[j])^2)
    
    }
  
}

p = plot_ly(z = loss2, 
            x = slope, 
            y = intercept, 
            width = 1000, 
            height = 1000) %>% 
    add_surface(contours = list(z = list(show = TRUE,
                                         usecolormap = TRUE,
                                         highlightcolor = "#ff0000",
                                         project = list(z = TRUE))))
```

<center><br>`r p`<br></center>


